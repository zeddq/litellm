INFO:     127.0.0.1:51704 - "GET /health HTTP/1.1" 200 OK
INFO:     127.0.0.1:51708 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:51711 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:51714 - "POST /v1/chat/completions HTTP/1.1" 200 OK
Initialized litellm callbacks, Async Success Callbacks: [<integrations.prisma_proxy.PrismaProxyLogger object at 0x11021d590>]
ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
Final returned optional params: {'stream': True}

Raw OpenAI Chunk
ModelResponseStream(id='U7AeadqqCveKvdIPo57okQQ', created=1763618899, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='I received your test message. I am functioning correctly and ready to assist you with your Python project using Poetry.\n\nWe currently have `config.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=365, prompt_tokens=867, total_tokens=1232, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=336, rejected_prediction_tokens=None, text_tokens=29), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=867, image_tokens=None)))

model_response finish reason 3: None; response_obj={'text': 'I received your test message. I am functioning correctly and ready to assist you with your Python project using Poetry.\n\nWe currently have `config.', 'is_finished': False, 'finish_reason': 'stop', 'logprobs': None, 'original_chunk': ModelResponseStream(id='U7AeadqqCveKvdIPo57okQQ', created=1763618899, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='I received your test message. I am functioning correctly and ready to assist you with your Python project using Poetry.\n\nWe currently have `config.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=365, prompt_tokens=867, total_tokens=1232, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=336, rejected_prediction_tokens=None, text_tokens=29), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=867, image_tokens=None))), 'usage': Usage(completion_tokens=365, prompt_tokens=867, total_tokens=1232, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=336, rejected_prediction_tokens=None, text_tokens=29), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=867, image_tokens=None))}
original delta: {'provider_specific_fields': None, 'content': 'I received your test message. I am functioning correctly and ready to assist you with your Python project using Poetry.\n\nWe currently have `config.', 'role': None, 'function_call': None, 'tool_calls': None, 'audio': None}
new delta: Delta(provider_specific_fields=None, content='I received your test message. I am functioning correctly and ready to assist you with your Python project using Poetry.\n\nWe currently have `config.', role=None, function_call=None, tool_calls=None, audio=None)
model_response.choices[0].delta: Delta(provider_specific_fields=None, content='I received your test message. I am functioning correctly and ready to assist you with your Python project using Poetry.\n\nWe currently have `config.', role=None, function_call=None, tool_calls=None, audio=None); completion_obj: {'content': 'I received your test message. I am functioning correctly and ready to assist you with your Python project using Poetry.\n\nWe currently have `config.'}
self.sent_first_chunk: False
completion_obj: {'content': 'I received your test message. I am functioning correctly and ready to assist you with your Python project using Poetry.\n\nWe currently have `config.'}, model_response.choices[0]: StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='I received your test message. I am functioning correctly and ready to assist you with your Python project using Poetry.\n\nWe currently have `config.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None), response_obj: {'text': 'I received your test message. I am functioning correctly and ready to assist you with your Python project using Poetry.\n\nWe currently have `config.', 'is_finished': False, 'finish_reason': 'stop', 'logprobs': None, 'original_chunk': ModelResponseStream(id='U7AeadqqCveKvdIPo57okQQ', created=1763618899, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='I received your test message. I am functioning correctly and ready to assist you with your Python project using Poetry.\n\nWe currently have `config.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=365, prompt_tokens=867, total_tokens=1232, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=336, rejected_prediction_tokens=None, text_tokens=29), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=867, image_tokens=None))), 'usage': Usage(completion_tokens=365, prompt_tokens=867, total_tokens=1232, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=336, rejected_prediction_tokens=None, text_tokens=29), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=867, image_tokens=None))}
choice_json: {'index': 0, 'delta': {'provider_specific_fields': None, 'content': 'I received your test message. I am functioning correctly and ready to assist you with your Python project using Poetry.\n\nWe currently have `config.', 'role': None, 'function_call': None, 'tool_calls': None, 'audio': None}, 'logprobs': None}
choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='I received your test message. I am functioning correctly and ready to assist you with your Python project using Poetry.\n\nWe currently have `config.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)]
final returned processed chunk: ModelResponseStream(id='U7AeadqqCveKvdIPo57okQQ', created=1763618899, model='gemini-3-pro-preview', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='I received your test message. I am functioning correctly and ready to assist you with your Python project using Poetry.\n\nWe currently have `config.', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])

Raw OpenAI Chunk
ModelResponseStream(id='U7AeadqqCveKvdIPo57okQQ', created=1763618899, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='yaml` open. How can I help you proceed?', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=376, prompt_tokens=867, total_tokens=1243, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=336, rejected_prediction_tokens=None, text_tokens=40), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=867, image_tokens=None)))

model_response finish reason 3: None; response_obj={'text': 'yaml` open. How can I help you proceed?', 'is_finished': False, 'finish_reason': 'stop', 'logprobs': None, 'original_chunk': ModelResponseStream(id='U7AeadqqCveKvdIPo57okQQ', created=1763618899, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='yaml` open. How can I help you proceed?', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=376, prompt_tokens=867, total_tokens=1243, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=336, rejected_prediction_tokens=None, text_tokens=40), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=867, image_tokens=None))), 'usage': Usage(completion_tokens=376, prompt_tokens=867, total_tokens=1243, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=336, rejected_prediction_tokens=None, text_tokens=40), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=867, image_tokens=None))}
original delta: {'provider_specific_fields': None, 'content': 'yaml` open. How can I help you proceed?', 'role': None, 'function_call': None, 'tool_calls': None, 'audio': None}
new delta: Delta(provider_specific_fields=None, content='yaml` open. How can I help you proceed?', role=None, function_call=None, tool_calls=None, audio=None)
model_response.choices[0].delta: Delta(provider_specific_fields=None, content='yaml` open. How can I help you proceed?', role=None, function_call=None, tool_calls=None, audio=None); completion_obj: {'content': 'yaml` open. How can I help you proceed?'}
self.sent_first_chunk: True
completion_obj: {'content': 'yaml` open. How can I help you proceed?'}, model_response.choices[0]: StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='yaml` open. How can I help you proceed?', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None), response_obj: {'text': 'yaml` open. How can I help you proceed?', 'is_finished': False, 'finish_reason': 'stop', 'logprobs': None, 'original_chunk': ModelResponseStream(id='U7AeadqqCveKvdIPo57okQQ', created=1763618899, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='yaml` open. How can I help you proceed?', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=376, prompt_tokens=867, total_tokens=1243, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=336, rejected_prediction_tokens=None, text_tokens=40), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=867, image_tokens=None))), 'usage': Usage(completion_tokens=376, prompt_tokens=867, total_tokens=1243, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=336, rejected_prediction_tokens=None, text_tokens=40), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=867, image_tokens=None))}
choice_json: {'index': 0, 'delta': {'provider_specific_fields': None, 'content': 'yaml` open. How can I help you proceed?', 'role': None, 'function_call': None, 'tool_calls': None, 'audio': None}, 'logprobs': None}
choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='yaml` open. How can I help you proceed?', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)]
final returned processed chunk: ModelResponseStream(id='U7AeadqqCveKvdIPo57okQQ', created=1763618899, model='gemini-3-pro-preview', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='yaml` open. How can I help you proceed?', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])

Raw OpenAI Chunk
ModelResponseStream(id='U7AeadqqCveKvdIPo57okQQ', created=1763618899, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content=None, role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=376, prompt_tokens=867, total_tokens=1243, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=336, rejected_prediction_tokens=None, text_tokens=40), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=867, image_tokens=None)))

model_response finish reason 3: None; response_obj={'text': '', 'is_finished': False, 'finish_reason': 'stop', 'logprobs': None, 'original_chunk': ModelResponseStream(id='U7AeadqqCveKvdIPo57okQQ', created=1763618899, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content=None, role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=376, prompt_tokens=867, total_tokens=1243, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=336, rejected_prediction_tokens=None, text_tokens=40), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=867, image_tokens=None))), 'usage': Usage(completion_tokens=376, prompt_tokens=867, total_tokens=1243, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=336, rejected_prediction_tokens=None, text_tokens=40), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=867, image_tokens=None))}
original delta: {'provider_specific_fields': None, 'content': None, 'role': None, 'function_call': None, 'tool_calls': None, 'audio': None}
new delta: Delta(provider_specific_fields=None, content=None, role=None, function_call=None, tool_calls=None, audio=None)
model_response.choices[0].delta: Delta(provider_specific_fields=None, content=None, role=None, function_call=None, tool_calls=None, audio=None); completion_obj: {'content': ''}
self.sent_first_chunk: True
completion_obj: {'content': ''}, model_response.choices[0]: StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=None, role=None, function_call=None, tool_calls=None, audio=None), logprobs=None), response_obj: {'text': '', 'is_finished': False, 'finish_reason': 'stop', 'logprobs': None, 'original_chunk': ModelResponseStream(id='U7AeadqqCveKvdIPo57okQQ', created=1763618899, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content=None, role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=376, prompt_tokens=867, total_tokens=1243, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=336, rejected_prediction_tokens=None, text_tokens=40), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=867, image_tokens=None))), 'usage': Usage(completion_tokens=376, prompt_tokens=867, total_tokens=1243, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=336, rejected_prediction_tokens=None, text_tokens=40), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=867, image_tokens=None))}
Logging Details LiteLLM-Async Success Call, cache_hit=False
Async success callbacks: Got a complete streaming response
INFO:     127.0.0.1:51741 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:51746 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:51749 - "POST /v1/chat/completions HTTP/1.1" 200 OK
Initialized litellm callbacks, Async Success Callbacks: [<integrations.prisma_proxy.PrismaProxyLogger object at 0x11021d590>, <litellm.integrations.opentelemetry.OpenTelemetry object at 0x110223a10>, 'cache']
ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None
Final returned optional params: {'stream': True}

Raw OpenAI Chunk
ModelResponseStream(id='E7EeaemrKMytkdUPgoCW-AY', created=1763619091, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='Test 2 received! I am still here and fully operational.\n\nI am ready to assist you with your **Python 3.13.8** project or any changes needed in `config.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=321, prompt_tokens=911, total_tokens=1232, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=280, rejected_prediction_tokens=None, text_tokens=41), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=911, image_tokens=None)))

model_response finish reason 3: None; response_obj={'text': 'Test 2 received! I am still here and fully operational.\n\nI am ready to assist you with your **Python 3.13.8** project or any changes needed in `config.', 'is_finished': False, 'finish_reason': 'stop', 'logprobs': None, 'original_chunk': ModelResponseStream(id='E7EeaemrKMytkdUPgoCW-AY', created=1763619091, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='Test 2 received! I am still here and fully operational.\n\nI am ready to assist you with your **Python 3.13.8** project or any changes needed in `config.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=321, prompt_tokens=911, total_tokens=1232, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=280, rejected_prediction_tokens=None, text_tokens=41), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=911, image_tokens=None))), 'usage': Usage(completion_tokens=321, prompt_tokens=911, total_tokens=1232, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=280, rejected_prediction_tokens=None, text_tokens=41), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=911, image_tokens=None))}
original delta: {'provider_specific_fields': None, 'content': 'Test 2 received! I am still here and fully operational.\n\nI am ready to assist you with your **Python 3.13.8** project or any changes needed in `config.', 'role': None, 'function_call': None, 'tool_calls': None, 'audio': None}
new delta: Delta(provider_specific_fields=None, content='Test 2 received! I am still here and fully operational.\n\nI am ready to assist you with your **Python 3.13.8** project or any changes needed in `config.', role=None, function_call=None, tool_calls=None, audio=None)
model_response.choices[0].delta: Delta(provider_specific_fields=None, content='Test 2 received! I am still here and fully operational.\n\nI am ready to assist you with your **Python 3.13.8** project or any changes needed in `config.', role=None, function_call=None, tool_calls=None, audio=None); completion_obj: {'content': 'Test 2 received! I am still here and fully operational.\n\nI am ready to assist you with your **Python 3.13.8** project or any changes needed in `config.'}
self.sent_first_chunk: False
completion_obj: {'content': 'Test 2 received! I am still here and fully operational.\n\nI am ready to assist you with your **Python 3.13.8** project or any changes needed in `config.'}, model_response.choices[0]: StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Test 2 received! I am still here and fully operational.\n\nI am ready to assist you with your **Python 3.13.8** project or any changes needed in `config.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None), response_obj: {'text': 'Test 2 received! I am still here and fully operational.\n\nI am ready to assist you with your **Python 3.13.8** project or any changes needed in `config.', 'is_finished': False, 'finish_reason': 'stop', 'logprobs': None, 'original_chunk': ModelResponseStream(id='E7EeaemrKMytkdUPgoCW-AY', created=1763619091, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='Test 2 received! I am still here and fully operational.\n\nI am ready to assist you with your **Python 3.13.8** project or any changes needed in `config.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=321, prompt_tokens=911, total_tokens=1232, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=280, rejected_prediction_tokens=None, text_tokens=41), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=911, image_tokens=None))), 'usage': Usage(completion_tokens=321, prompt_tokens=911, total_tokens=1232, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=280, rejected_prediction_tokens=None, text_tokens=41), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=911, image_tokens=None))}
choice_json: {'index': 0, 'delta': {'provider_specific_fields': None, 'content': 'Test 2 received! I am still here and fully operational.\n\nI am ready to assist you with your **Python 3.13.8** project or any changes needed in `config.', 'role': None, 'function_call': None, 'tool_calls': None, 'audio': None}, 'logprobs': None}
choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Test 2 received! I am still here and fully operational.\n\nI am ready to assist you with your **Python 3.13.8** project or any changes needed in `config.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)]
final returned processed chunk: ModelResponseStream(id='E7EeaemrKMytkdUPgoCW-AY', created=1763619091, model='gemini-3-pro-preview', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Test 2 received! I am still here and fully operational.\n\nI am ready to assist you with your **Python 3.13.8** project or any changes needed in `config.', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])

Raw OpenAI Chunk
ModelResponseStream(id='E7EeaemrKMytkdUPgoCW-AY', created=1763619091, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='yaml`. Just let me know what you need to do.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=333, prompt_tokens=911, total_tokens=1244, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=280, rejected_prediction_tokens=None, text_tokens=53), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=911, image_tokens=None)))

model_response finish reason 3: None; response_obj={'text': 'yaml`. Just let me know what you need to do.', 'is_finished': False, 'finish_reason': 'stop', 'logprobs': None, 'original_chunk': ModelResponseStream(id='E7EeaemrKMytkdUPgoCW-AY', created=1763619091, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='yaml`. Just let me know what you need to do.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=333, prompt_tokens=911, total_tokens=1244, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=280, rejected_prediction_tokens=None, text_tokens=53), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=911, image_tokens=None))), 'usage': Usage(completion_tokens=333, prompt_tokens=911, total_tokens=1244, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=280, rejected_prediction_tokens=None, text_tokens=53), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=911, image_tokens=None))}
original delta: {'provider_specific_fields': None, 'content': 'yaml`. Just let me know what you need to do.', 'role': None, 'function_call': None, 'tool_calls': None, 'audio': None}
new delta: Delta(provider_specific_fields=None, content='yaml`. Just let me know what you need to do.', role=None, function_call=None, tool_calls=None, audio=None)
model_response.choices[0].delta: Delta(provider_specific_fields=None, content='yaml`. Just let me know what you need to do.', role=None, function_call=None, tool_calls=None, audio=None); completion_obj: {'content': 'yaml`. Just let me know what you need to do.'}
self.sent_first_chunk: True
completion_obj: {'content': 'yaml`. Just let me know what you need to do.'}, model_response.choices[0]: StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='yaml`. Just let me know what you need to do.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None), response_obj: {'text': 'yaml`. Just let me know what you need to do.', 'is_finished': False, 'finish_reason': 'stop', 'logprobs': None, 'original_chunk': ModelResponseStream(id='E7EeaemrKMytkdUPgoCW-AY', created=1763619091, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content='yaml`. Just let me know what you need to do.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=333, prompt_tokens=911, total_tokens=1244, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=280, rejected_prediction_tokens=None, text_tokens=53), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=911, image_tokens=None))), 'usage': Usage(completion_tokens=333, prompt_tokens=911, total_tokens=1244, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=280, rejected_prediction_tokens=None, text_tokens=53), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=911, image_tokens=None))}
choice_json: {'index': 0, 'delta': {'provider_specific_fields': None, 'content': 'yaml`. Just let me know what you need to do.', 'role': None, 'function_call': None, 'tool_calls': None, 'audio': None}, 'logprobs': None}
choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='yaml`. Just let me know what you need to do.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)]
final returned processed chunk: ModelResponseStream(id='E7EeaemrKMytkdUPgoCW-AY', created=1763619091, model='gemini-3-pro-preview', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='yaml`. Just let me know what you need to do.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, citations=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[])

Raw OpenAI Chunk
ModelResponseStream(id='E7EeaemrKMytkdUPgoCW-AY', created=1763619091, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content=None, role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=333, prompt_tokens=911, total_tokens=1244, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=280, rejected_prediction_tokens=None, text_tokens=53), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=911, image_tokens=None)))

model_response finish reason 3: None; response_obj={'text': '', 'is_finished': False, 'finish_reason': 'stop', 'logprobs': None, 'original_chunk': ModelResponseStream(id='E7EeaemrKMytkdUPgoCW-AY', created=1763619091, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content=None, role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=333, prompt_tokens=911, total_tokens=1244, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=280, rejected_prediction_tokens=None, text_tokens=53), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=911, image_tokens=None))), 'usage': Usage(completion_tokens=333, prompt_tokens=911, total_tokens=1244, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=280, rejected_prediction_tokens=None, text_tokens=53), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=911, image_tokens=None))}
original delta: {'provider_specific_fields': None, 'content': None, 'role': None, 'function_call': None, 'tool_calls': None, 'audio': None}
new delta: Delta(provider_specific_fields=None, content=None, role=None, function_call=None, tool_calls=None, audio=None)
model_response.choices[0].delta: Delta(provider_specific_fields=None, content=None, role=None, function_call=None, tool_calls=None, audio=None); completion_obj: {'content': ''}
self.sent_first_chunk: True
completion_obj: {'content': ''}, model_response.choices[0]: StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=None, role=None, function_call=None, tool_calls=None, audio=None), logprobs=None), response_obj: {'text': '', 'is_finished': False, 'finish_reason': 'stop', 'logprobs': None, 'original_chunk': ModelResponseStream(id='E7EeaemrKMytkdUPgoCW-AY', created=1763619091, model=None, object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content=None, role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_ratings=[], vertex_ai_citation_metadata=[], usage=Usage(completion_tokens=333, prompt_tokens=911, total_tokens=1244, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=280, rejected_prediction_tokens=None, text_tokens=53), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=911, image_tokens=None))), 'usage': Usage(completion_tokens=333, prompt_tokens=911, total_tokens=1244, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=280, rejected_prediction_tokens=None, text_tokens=53), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=911, image_tokens=None))}
Logging Details LiteLLM-Async Success Call, cache_hit=False
Async success callbacks: Got a complete streaming response
INFO:     127.0.0.1:51759 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:51768 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:51772 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:51831 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:51843 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:51855 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:51873 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:51877 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:51905 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:51909 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:51917 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:51932 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:51951 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:51960 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:51972 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:51984 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:52000 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:52041 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:52061 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:52331 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:52499 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:52531 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:52550 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:52652 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:52666 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:52674 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:52684 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:52700 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:52720 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:52753 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:52769 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:52803 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:52853 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:52873 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:52882 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:52898 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:52906 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:52933 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:52953 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:53017 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:53045 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:53196 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:53318 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:53337 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:53351 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:53366 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:53384 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:53405 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:53427 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:53443 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:53455 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:53473 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:53488 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:53511 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:53524 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:53537 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:53552 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:53572 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:53604 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:53634 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:53662 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:53675 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:53693 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:53708 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:53724 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:53735 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:53751 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:53769 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:53789 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:53803 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:53813 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:53826 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:53843 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:53851 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:53954 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:53966 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:53974 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:53981 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:53991 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54007 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54032 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54055 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54067 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54078 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54083 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54089 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54095 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54102 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54116 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54136 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54156 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54168 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54173 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54177 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54187 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54199 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54207 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54210 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54227 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54236 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54239 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54246 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54266 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54283 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54297 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54309 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54329 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54339 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54349 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54361 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54384 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54399 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54411 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54424 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54436 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54447 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54466 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54478 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54489 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54501 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54522 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54536 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54551 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54567 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54581 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54595 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54611 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54625 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54639 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54657 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54665 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54671 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54674 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54678 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54693 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54714 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54717 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54722 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54726 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54738 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54741 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54745 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54758 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54762 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54765 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54769 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54772 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54784 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54787 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54794 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54799 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54808 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54812 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54816 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54819 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54823 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54826 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54833 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54837 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54841 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54844 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54851 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54857 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54861 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54870 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54873 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54877 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54882 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54885 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54888 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54892 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54897 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54902 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54907 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54924 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54927 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54936 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54939 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54947 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54951 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54955 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54959 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54967 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54980 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54986 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:54992 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55000 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55003 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55008 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55014 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55023 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55031 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55044 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55056 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55059 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55066 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55073 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55076 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55079 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55089 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55098 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55103 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55106 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55114 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55126 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55129 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55132 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55137 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55141 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55159 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55166 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55172 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55175 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55178 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55182 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55186 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55190 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55209 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55219 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55223 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55226 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55233 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55239 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55251 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55269 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55274 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55280 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55283 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55291 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55303 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55309 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55324 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55338 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55350 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55353 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55357 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55361 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55365 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55368 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55374 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55377 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55382 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55386 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55389 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55393 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55397 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55403 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55415 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55433 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55449 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55464 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55469 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55473 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55513 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55524 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55542 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55554 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55609 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55674 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55742 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55821 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55894 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:55974 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:56006 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:56107 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:56113 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:56149 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:56166 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:56182 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:56202 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:56205 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:56209 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:56248 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:56257 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:56272 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:56288 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:56404 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:56445 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:56466 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:56477 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:56511 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:56530 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:56545 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:56567 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:56607 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:56622 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:56626 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:56629 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:56707 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:56758 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:56764 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:56808 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:56842 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:56846 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:56863 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:56868 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:56877 - "GET /v1/models HTTP/1.1" 200 OK
INFO:     127.0.0.1:56911 - "GET /v1/models HTTP/1.1" 200 OK
