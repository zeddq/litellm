general_settings:
  master_key: sk-1234
  forward_openai_org_id: true
  # Forward all client headers globally (recommended for Supermemory)
  forward_client_headers_to_llm_api: true
  database_url: os.environ/DATABASE_URL
  database_connection_pool_limit: 100
  store_model_in_db: true
  store_prompts_in_spend_logs: true

# User ID mappings for Supermemory routing
user_id_mappings:
  # Custom header for explicit user ID specification
  custom_header: "x-memory-user-id"

  # Pattern-based detection from existing headers
  header_patterns:
    - header: "user-agent"
      pattern: "^OpenAIClientImpl/Java"
      user_id: "secretary-dev"

    - header: "user-agent"
      pattern: "^anthropic-sdk-python"
      user_id: "claude-code"

    - header: "user-agent"
      pattern: "^Claude Code"
      user_id: "claude-code-cli"

  # Default user ID when no pattern matches
  default_user_id: "default-dev"

# Tool execution configuration for automatic tool calling
# Enables the proxy to automatically execute tool calls from LLMs
tool_execution:
  # Enable/disable tool execution globally
  enabled: true

  # Maximum number of tool execution loops to prevent infinite loops
  max_iterations: 10

  # Timeout for each tool execution (seconds)
  timeout_per_tool: 30.0

  # Supermemory API key for tool execution (use environment variable for security)
  supermemory_api_key: os.environ/SUPERMEMORY_API_KEY

  # Supermemory API base URL
  supermemory_base_url: https://api.supermemory.ai

  # Maximum number of search results to return
  max_results: 5

# Context retrieval configuration for Supermemory integration
# Enables automatic retrieval and injection of relevant context from Supermemory
context_retrieval:
  # Enable/disable context retrieval globally
  enabled: true

  # Supermemory API key (use environment variable for security)
  api_key: os.environ/SUPERMEMORY_API_KEY

  # Supermemory API base URL
  base_url: https://api.supermemory.ai

  # Query extraction strategy:
  # - last_user: Use only the last user message as query
  # - first_user: Use only the first user message as query
  # - all_user: Concatenate all user messages as query
  # - last_assistant: Use the last assistant message as query (for follow-ups)
  query_strategy: last_user

  # Context injection strategy:
  # - system: Inject as a system message (recommended for most models)
  # - user_prefix: Prepend to the first user message
  # - user_suffix: Append to the last user message
  injection_strategy: system

  # Container tag for Supermemory organization
  container_tag: supermemory

  # Maximum length of retrieved context in characters (100-100000)
  max_context_length: 4000

  # Maximum number of context results to retrieve (1-20)
  max_results: 5

  # API request timeout in seconds (1.0-60.0)
  timeout: 10.0

  # Enable context retrieval only for specific models (whitelist approach)
  # If specified, only these models will use context retrieval
  # Leave empty or null to enable for all models
  enabled_for_models:
    - claude-sonnet-4.5
    - claude-haiku-4.5

  # Disable context retrieval for specific models (blacklist approach)
  # Cannot be used together with enabled_for_models
  # disabled_for_models:
  #   - gpt-5-pro


# This section defines the models that your proxy will expose.
model_list:
  - model_name: gpt-5-pro
    litellm_params:
      model: openai/gpt-5-pro
      api_key: os.environ/OPENAI_API_KEY

  # Alias: gpt-5-codex -> same as gpt
  - model_name: gpt-5-codex
    litellm_params:
      model: openai/gpt-5-codex
      api_key: os.environ/OPENAI_API_KEY

  # Alias: gpt -> gpt-5-codex
  - model_name: gpt
    litellm_params:
      model: openai/gpt-5-codex
      api_key: os.environ/OPENAI_API_KEY

  # Primary: claude-sonnet-4.5 with Supermemory integration
  - model_name: claude-sonnet-4.5
    litellm_params:
      api_base: https://api.supermemory.ai/v3/api.anthropic.com
      model: anthropic/claude-sonnet-4-5-20250929
      api_key: os.environ/ANTHROPIC_API_KEY
      custom_llm_provider: anthropic
      extra_headers:
        x-supermemory-api-key: os.environ/SUPERMEMORY_API_KEY
      thinking: {
        "type": "enabled",
        "budget_tokens": 4096
      }

  # Alias: sonnet -> claude-sonnet-4.5
  - model_name: sonnet
    litellm_params:
      model: anthropic/claude-sonnet-4-5-20250929
      api_key: os.environ/ANTHROPIC_API_KEY
      thinking: {
        "type": "enabled",
        "budget_tokens": 4096
      }

  - model_name: claude-haiku-4.5
    litellm_params:
      api_base: https://api.supermemory.ai/v3/api.anthropic.com
      model: anthropic/claude-haiku-4-5-20251001
      api_key: os.environ/ANTHROPIC_API_KEY
      custom_llm_provider: anthropic
      extra_headers:
        x-supermemory-api-key: "sm_QcByjXoSBWceAnA8PB5sde_EddoiUyplQTGJjTksHjPkwrxnVVHYQUlEXGLQCbAnQDWyHNxtVMlKbQiszUznRfk"

mcp_servers:
  jetbrains_mcp:
    transport: sse
    url: "http://localhost:64342/sse"
    auth_type: "none"

# General settings for the proxy server
litellm_settings:
  # PostgreSQL Configuration
  database_type: "prisma"
  database_url: os.environ/DATABASE_URL
  store_model_in_db: true

  # CRITICAL: Enable callbacks for DB storage AND OTEL
  success_callback: ["postgres", "otel"]
  failure_callback: ["postgres", "otel"]

  # OTEL Configuration
  otel: true
  otel_exporter: "otlp_http"
  otel_endpoint: "http://localhost:4318/v1/traces"
  otel_service_name: "litellm-proxy"
  otel_headers: ""

  # Redis Cache Configuration
  cache: true
  cache_params:
    type: redis
    ttl: 3600
    host: localhost
    port: 6379
    password: sk-1234

  # MCP Configuration
  mcp_aliases:
    "jetbrains": "jetbrains_mcp"

  # Additional logging
  set_verbose: true
  json_logs: true
  drop_params: true
  # Forward traceparent for distributed tracing
  forward_traceparent_to_llm_provider: true

